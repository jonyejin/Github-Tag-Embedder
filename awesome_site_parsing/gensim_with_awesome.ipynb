{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# common_text에는 파싱된 워드 리스트들이 들어가 있음.\n",
    "from gensim.test.utils import common_texts\n",
    "# Doc2Vec이 우리가 텍스트를 사용해서 학습되는 모델(뉴럴넷)이고 \n",
    "# TaggedDocument가 넘겨주는 텍스트들. \n",
    "# 여기서, corpus와 ID들을 함께 넘겨줘야 하는데, 여기서 ID는 tag와 같은 말임\n",
    "# Q: 여기서, 여러 tag를 함께 넘겨준 다음, 적합한 태그를 찾아주는 방식으로도 처리할 수 있는지 파악하는 것이 필요함. \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# import하기\n",
    "import json\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('./../github_crawling/READMEs.json')\n",
    "awesome_file = json.load(f)\n",
    "\n",
    "# TaggedDocument는 해당 corpus의 id를 함께 넘겨주는 것을 말합니다. \n",
    "# 아래 코드에서 doc에는 단어의 묶음이, tags에는 해당 문서를 표현하는 태그가 들어가게 됨. \n",
    "# 흠, 그렇다면, 이 태그에 고유 id가 아닌 다른 것을 넣어주면 현재 문서에 맞는 tag를 찾아주기도 하나? \n",
    "# 이렇게 할 경우, words에는 문서를, tags에는 키워드를 넣고, words에서 tags를 예측하는 짓을 할 수도 있지 않을까? \n",
    "\n",
    "common_texts_and_tags = [\n",
    "    (key, value) for key, value in awesome_file.items()\n",
    "]\n",
    "\n",
    "print(len(common_texts_and_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"##\"*20)\n",
    "# print(\"tags and its texts\")\n",
    "# print(\"##\"*20)\n",
    "# for text, tag in common_texts_and_tags:\n",
    "#     print(f\"tag: {tag}, text: {text}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# text: 단어별로 분할되어 있는 리스트 \n",
    "# tags: 해당 문서를 의미하는 태그. 여기서, tags를 unique id로 넣기는 했는데, 그렇지 않게 넣어서 학습시키는 것도 가능할 것으로 보임. \n",
    "TRAIN_documents = [TaggedDocument(words=value, tags=key.split(\" \")) for key, value in common_texts_and_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training. \n",
    "# 여러 Parameter들을 사용하여 튜닝할 수 있음. \n",
    "model = Doc2Vec(TRAIN_documents, vector_size=100, window=100, epochs=1, min_count=0, workers=4)\n",
    "model.build_vocab(TRAIN_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewwonwhoonah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('openpose', 0.16198016703128815), ('human-pose', 0.15649254620075226), ('list', 0.15439288318157196), ('multi-person', 0.12490002065896988), ('hand-estimation', 0.12390804290771484)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "# python3 -c \"import nltk; nltk.download('all')\"\n",
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"new readme\")\n",
    "print(model.dv.most_similar(positive=[model.infer_vector(test_data)],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "K-Means Clustering\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/f1h284rn02128q30qhbmyqn80000gn/T/ipykernel_5065/1188204071.py:7: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  X = model.docvecs.vectors_docs # document vector 전체를 가져옴.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'vectors_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m##\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m Clustering_Method \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdocvecs\u001b[39m.\u001b[39;49mvectors_docs \u001b[39m# document vector 전체를 가져옴. \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m Clustering_Method\u001b[39m.\u001b[39mfit(X)\u001b[39m# fitting \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 결과를 보면 알겠지만, 생각보다 클러스터링이 잘 되지 않음. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yejin/Github-Tag-Embedder/awesome_site_parsing/gensim_with_awesome.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 일단은 이것 또한 트레이닝 셋이 적어서 그런 것으로 보임. \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'vectors_docs'"
     ]
    }
   ],
   "source": [
    "\n",
    "# document clustering \n",
    "\n",
    "print(\"##\"*30)\n",
    "print(\"K-Means Clustering\")\n",
    "print(\"##\"*30)\n",
    "Clustering_Method = KMeans(n_clusters=2, random_state=0)\n",
    "X = model.docvecs.vectors_docs # document vector 전체를 가져옴. \n",
    "Clustering_Method.fit(X)# fitting \n",
    "# 결과를 보면 알겠지만, 생각보다 클러스터링이 잘 되지 않음. \n",
    "# 일단은 이것 또한 트레이닝 셋이 적어서 그런 것으로 보임. \n",
    "cluster_dict = {i:[] for i in range(0, 2)}\n",
    "for text_tags, label in zip(common_texts_and_tags, Clustering_Method.labels_):\n",
    "    text, tags = text_tags\n",
    "    cluster_dict[label].append(text)\n",
    "for label, lst in cluster_dict.items():\n",
    "    print(f\"Cluster {label}\")\n",
    "    for x in lst:\n",
    "        print(x)\n",
    "    print(\"--\"*30)\n",
    "print(\"##\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40fc0dabaa44856a7ea2aaace631e92d758e85cb3e5626f690455fc0714b666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
