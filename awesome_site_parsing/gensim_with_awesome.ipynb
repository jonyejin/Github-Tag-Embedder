{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# common_text에는 파싱된 워드 리스트들이 들어가 있음.\n",
    "from gensim.test.utils import common_texts\n",
    "# Doc2Vec이 우리가 텍스트를 사용해서 학습되는 모델(뉴럴넷)이고 \n",
    "# TaggedDocument가 넘겨주는 텍스트들. \n",
    "# 여기서, corpus와 ID들을 함께 넘겨줘야 하는데, 여기서 ID는 tag와 같은 말임\n",
    "# Q: 여기서, 여러 tag를 함께 넘겨준 다음, 적합한 태그를 찾아주는 방식으로도 처리할 수 있는지 파악하는 것이 필요함. \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# import하기\n",
    "import json\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('./doc_output.json')\n",
    "awesome_file = json.load(f)\n",
    "\n",
    "# TaggedDocument는 해당 corpus의 id를 함께 넘겨주는 것을 말합니다. \n",
    "# 아래 코드에서 doc에는 단어의 묶음이, tags에는 해당 문서를 표현하는 태그가 들어가게 됨. \n",
    "# 흠, 그렇다면, 이 태그에 고유 id가 아닌 다른 것을 넣어주면 현재 문서에 맞는 tag를 찾아주기도 하나? \n",
    "# 이렇게 할 경우, words에는 문서를, tags에는 키워드를 넣고, words에서 tags를 예측하는 짓을 할 수도 있지 않을까? \n",
    "\n",
    "common_texts_and_tags = [\n",
    "    (key, value) for key, value in awesome_file.items()\n",
    "]\n",
    "\n",
    "#key 관리하기\n",
    "tags = [i[0] for i in common_texts_and_tags]\n",
    "\n",
    "print(len(common_texts_and_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"##\"*20)\n",
    "# print(\"tags and its texts\")\n",
    "# print(\"##\"*20)\n",
    "# for text, tag in common_texts_and_tags:\n",
    "#     print(f\"tag: {tag}, text: {text}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# text: 단어별로 분할되어 있는 리스트 \n",
    "# tags: 해당 문서를 의미하는 태그. 여기서, tags를 unique id로 넣기는 했는데, 그렇지 않게 넣어서 학습시키는 것도 가능할 것으로 보임. \n",
    "TRAIN_documents = [TaggedDocument(words=value, tags=key.split(\" \")) for key, value in common_texts_and_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training. \n",
    "# 여러 Parameter들을 사용하여 튜닝할 수 있음. \n",
    "model = Doc2Vec(TRAIN_documents, vector_size=100, window=100, epochs=1000, min_count=2, workers=8)\n",
    "model.build_vocab(TRAIN_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_tmpfile\n\u001b[0;32m      2\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./my_doc2vec_model3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model\u001b[39m.\u001b[39msave(fname)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = \"./my_doc2vec_model3\"\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./repo2vec_model3\"\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.267573564666591\n"
     ]
    }
   ],
   "source": [
    "print(model.total_train_time/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_in_doc_list(doc_list):\n",
    "  keys = {}\n",
    "  counts = dict()\n",
    "  \n",
    "  f = open('./doc_id_matching.json')\n",
    "  id_matching = json.load(f)\n",
    "  # print(f'similar docs list : {doc_list}')\n",
    "  for (key,value) in doc_list:\n",
    "    keys[key] = id_matching[key]\n",
    "  for words in keys.values():\n",
    "    for word in words.strip().split(\" \"):\n",
    "      word = word.replace('-', \" \")\n",
    "      if word in counts:\n",
    "          counts[word] += 1\n",
    "      else:\n",
    "          counts[word] = 1\n",
    "  for key, value in counts.items():\n",
    "    if value >= 2 :\n",
    "      print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data visualization\n",
      "data science\n",
      "evolutionary algorithm\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "test_data = word_tokenize(\"machine learning algorithm with scikit learn keras this repository contains various machine learning algorithm implemented scikit learn machine learning algorithm such supervised unsupervised simple reinforcement learning sentiment analysis natural language processing supervised simple deep learning algorithm dimensionality reduction bagging boosting implemented scikit learn keras numpy pandas matplotlib tutorials implementation notebook files supervised learning algorithm regression algorithm linear regression multivariate linear regression polynomial regression support vector machine decision trees random forest evaluating regression models using regularization classification algorithm logistic regression nearest neighbour support vector machine kernel support vector machine naive bayes decision trees random forest evaluating classification models unsupervised learning algorithm clustering algorithm means clustering heirarchical clustering association rule learning frequent itemset mining apriori eclat reinforcement learning multi armed bandit upper confidence bound thompson sampling natural language processing simple sentiment analysis using nltk deep learning simple artificial neural network using keras convolutional neural network using keras dimensionality reduction implemented section numpy pandas matplotlib others ipynb principal component analysis linear discriminant analysis kernel pricipal component analysis model selection bagging boosting grid search fold cross validation xgboost\")\n",
    "doc_list = model.dv.most_similar(positive=[model.infer_vector(test_data)],topn=5)\n",
    "find_key_in_doc_list(doc_list=doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UC\\AppData\\Local\\Temp\\ipykernel_1376\\1188204071.py:7: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  X = model.docvecs.vectors_docs # document vector 전체를 가져옴.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "K-Means Clustering\n",
      "############################################################\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'vectors_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m##\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m      6\u001b[0m Clustering_Method \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m X \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdocvecs\u001b[39m.\u001b[39;49mvectors_docs \u001b[39m# document vector 전체를 가져옴. \u001b[39;00m\n\u001b[0;32m      8\u001b[0m Clustering_Method\u001b[39m.\u001b[39mfit(X)\u001b[39m# fitting \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# 결과를 보면 알겠지만, 생각보다 클러스터링이 잘 되지 않음. \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# 일단은 이것 또한 트레이닝 셋이 적어서 그런 것으로 보임. \u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'vectors_docs'"
     ]
    }
   ],
   "source": [
    "\n",
    "# document clustering \n",
    "\n",
    "print(\"##\"*30)\n",
    "print(\"K-Means Clustering\")\n",
    "print(\"##\"*30)\n",
    "Clustering_Method = KMeans(n_clusters=2, random_state=0)\n",
    "X = model.docvecs.vectors_docs # document vector 전체를 가져옴. \n",
    "Clustering_Method.fit(X)# fitting \n",
    "# 결과를 보면 알겠지만, 생각보다 클러스터링이 잘 되지 않음. \n",
    "# 일단은 이것 또한 트레이닝 셋이 적어서 그런 것으로 보임. \n",
    "cluster_dict = {i:[] for i in range(0, 2)}\n",
    "for text_tags, label in zip(common_texts_and_tags, Clustering_Method.labels_):\n",
    "    text, tags = text_tags\n",
    "    cluster_dict[label].append(text)\n",
    "for label, lst in cluster_dict.items():\n",
    "    print(f\"Cluster {label}\")\n",
    "    for x in lst:\n",
    "        print(x)\n",
    "    print(\"--\"*30)\n",
    "print(\"##\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfb99ec84e88f276f6c4b06b4ba55c10dcd85b716ad93c8fd19ad772e11b3752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
