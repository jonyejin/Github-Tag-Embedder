{
	"data-science": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n",
	"deep-learning": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n",
	"machine-learning": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n",
	"neural-networks": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n",
	"python": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n",
	"tensorflow": "# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\nThis repository hosts the development of the Keras library.\nRead the documentation at [keras.io](https://keras.io/).\n\n## About Keras\n\nKeras is a deep learning API written in Python,\nrunning on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).\nIt was developed with a focus on enabling fast experimentation.\n*Being able to go from idea to result as fast as possible is key to doing good research.*\n\nKeras is:\n\n-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n    to free you to focus on the parts of the problem that really matter.\n-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n    complexity*: simple workflows should be quick and easy, while arbitrarily\n    advanced workflows should be *possible* via a clear path that builds upon\n    what you've already learned.\n-   **Powerful** -- Keras provides industry-strength performance and\n    scalability: it is used by organizations and companies including NASA,\n    YouTube, and Waymo.\n\n---\n\n## Keras & TensorFlow 2\n\n[TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.\nYou can think of it as an infrastructure layer for\n[differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).\nIt combines four key abilities:\n\n- Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n- Computing the gradient of arbitrary differentiable expressions.\n- Scaling computation to many devices, such as clusters of hundreds of GPUs.\n- Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n\nKeras is the high-level API of TensorFlow 2: an approachable, highly-productive interface\nfor solving machine learning problems,\nwith a focus on modern deep learning. It provides essential abstractions and building blocks for developing\nand shipping machine learning solutions with high iteration velocity.\n\nKeras empowers engineers and researchers to take full advantage of the scalability\nand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,\nand you can export your Keras models to run in the browser or on a mobile device.\n\n---\n\n## First contact with Keras\n\nThe core data structures of Keras are __layers__ and __models__.\nThe simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.\nFor more complex architectures, you should use the [Keras functional API](/guides/functional_api/),\nwhich allows you to build arbitrary graphs of layers or [write models entirely from scratch via subclassing](/guides/making_new_layers_and_models_via_subclassing/).\n\nHere is the `Sequential` model:\n\n```python\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom tensorflow.keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,\nwhile allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).\n\n```python\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nEvaluate your test loss and metrics in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nWhat you just saw is the most elementary way to use Keras.\n\nHowever, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.\nKeras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,\nyet it makes it possible to handle arbitrarily advanced use cases,\nonly requiring incremental learning at each step.\n\nIn much the same way that you were able to train & evaluate a simple neural network above in a few lines,\nyou can use Keras to quickly develop new training procedures or exotic model architectures.\nHere's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:\n\n```python\nimport tensorflow as tf\n\n# Prepare an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n# Prepare a loss function.\nloss_fn = tf.keras.losses.kl_divergence\n\n# Iterate over the batches of a dataset.\nfor inputs, targets in dataset:\n    # Open a GradientTape.\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        predictions = model(inputs)\n        # Compute the loss value for this batch.\n        loss_value = loss_fn(targets, predictions)\n\n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    # Update the weights of the model.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n```\n\nFor more in-depth tutorials about Keras, you can check out:\n\n-   [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n-   [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-   [Developer guides](https://keras.io/guides/)\n-   [Other learning resources](https://keras.io/getting_started/learning_resources/)\n\n---\n\n## Installation\n\nKeras comes packaged with TensorFlow 2 as `tensorflow.keras`.\nTo start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).\n\n---\n\n## Release and compatibility\n\nKeras has **nightly releases** (`keras-nightly` on PyPI)\nand **stable releases** (`keras` on PyPI).\nThe nightly Keras releases are usually compatible with the corresponding version\nof the `tf-nightly` releases\n(e.g. `keras-nightly==2.7.0.dev2021100607` should be\nused with `tf-nightly==2.7.0.dev2021100607`).\nWe don't maintain backward compatibility for nightly releases.\nFor stable releases, each Keras\nversion maps to a specific stable version of TensorFlow.\n\nThe table below shows the compatibility version mapping\nbetween TensorFlow versions and Keras versions.\n\nAll the release branches can be found on [GitHub](https://github.com/keras-team/keras/releases).\n\nAll the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).\n\n| Keras release | Note      | Compatible Tensorflow version |\n| -----------   | ----------- | -----------        |\n| [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5\n| 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6\n| [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7\n| [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8\n| nightly|                                            | tf-nightly\n\n---\n## Support\n\nYou can ask questions and join the development discussion:\n\n- In the [TensorFlow forum](https://discuss.tensorflow.org/).\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n\n---\n\n## Opening an issue\n\nYou can also post **bug reports and feature requests** (only)\nin [GitHub issues](https://github.com/keras-team/keras/issues).\n\n\n---\n\n## Opening a PR\n\nWe welcome contributions! Before opening a PR, please read\n[our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),\nand the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md).\n"
}